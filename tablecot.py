# -*- coding: utf-8 -*-
"""TableCoT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wh-nBqyrbbtAj91rwE7Eu1Tz4oq4UK8O
"""

!pip install -q -U google-generativeai

# Import the Python SDK
import google.generativeai as genai
# Used to securely store your API key
from google.colab import userdata
import os
import json
import time
import argparse
from pathlib import Path


GOOGLE_API_KEY="AIzaSyB0cy_y8IjPzTh8VdbuGua-IJb8-MDLCLg"
genai.configure(api_key=GOOGLE_API_KEY)

model = genai.GenerativeModel('gemini-2.0-flash-001')

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files

uploaded = files.upload()

DEMO = Path("demo_cot.txt").read_text()

def normalize(text: str) -> str:
    return text.strip().lower().rstrip(".")

def call_gemini(prompt: str) -> str:
    resp = model.generate_content(
        contents=prompt
    )
    return resp.text

def build_prompt(entry: dict) -> str:
     block     = entry.get("table_block", "")
     reasoning = entry.get("reasoning", "")

     prompt = DEMO + "\n"
     prompt += "Read the tables below and answer the question.\n\n"
     prompt += block + "\n"
     prompt += "Question: " + entry["question"] + "\n"
     prompt += "Please show your step-by-step reasoning, and at the *very end* include exactly one line of the form:\n"
     prompt += "`Final Answer: <your answer>`\n"
     return prompt

def load_examples(path: str, start: int, end: int):
    data = json.load(open(path))
    keys = list(data.keys())[start:end or None]
    return [(k,data[k]) for k in keys]

JSON_PATH   = "/content/b_od_004.json"
START, END  = 0, 0
SLEEP_SEC   = 0.5

"""## Generate text"""

examples = load_examples(JSON_PATH, START, END)
total, correct = len(examples), 0
results = []

for idx, (qid, entry) in enumerate(examples, 1):
    prompt = build_prompt(entry)
    print("Prompt given: ", prompt)
    full_pred = call_gemini(prompt)

    # ——— extract reasoning vs. final answer ———
    if "Final Answer:" in full_pred:
        reasoning_text, answer_line = full_pred.rsplit("Final Answer:", 1)
        reasoning = reasoning_text.strip()
        pred = answer_line.strip()
    else:
        # fallback if model didn’t follow format
        reasoning = ""
        pred = full_pred.strip()

    # ——— evaluation ———
    gts = entry["answer"]
    if not isinstance(gts, list):
        gts = [gts]
    hit = any(normalize(pred) == normalize(gt) for gt in gts)
    correct += hit

    # ——— record results ———
    results.append({
        "id": qid,
        "full_prediction": full_pred,  # raw output (with reasoning)
        "reasoning": reasoning,        # extracted CoT
        "prediction": pred,            # just the final answer
        "ground_truth": gts,
        "hit": hit
    })

    print(f"[{idx}/{total}] {'✓' if hit else '✗'}  → {pred}")
    time.sleep(SLEEP_SEC)

# summary
acc = correct / total if total else 0
print(f"\nAccuracy: {correct}/{total} = {acc:.2%}")

import pandas as pd

# results = [...]  # your list of dicts from the loop

df = pd.DataFrame(results)
# flatten the ground-truth list
df['ground_truth'] = df['ground_truth'].apply(lambda gt: ', '.join(gt))
# map hit to ✓/✗
df['Correct?'] = df['hit'].apply(lambda x: '✓' if x else '✗')

# select & rename columns
df = df[['id', 'ground_truth', 'prediction', 'Correct?']]
df.columns = ['Question ID', 'Expected Answer(s)', 'Model Prediction', 'Correct?']

# print as a GitHub-style Markdown table
print(df.to_markdown(index=False))
print(f"\nAccuracy: {correct}/{total} = {acc:.2%}")